{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disco  \n",
    "Copyright (C) 2022-present NAVER Corp.  \n",
    "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expressing Preferences via an EBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to express our preferences on the generated sequences. We do this via an EBM obtained by constraining a base model with scoring features.\n",
    "We're going to reuse the amazing example from the [README](README.md) to go in a little more depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Constraint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first revisit the amazing example from the [README](README.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `BooleanScorer`, it's straightforward to express that we want to have \"amazing\" in the sampled texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.scorers import BooleanScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_scorer = BooleanScorer(lambda s, c: \"amazing\" in s.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already (log-)score text samples, using a named tuple defined in disco to format them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions.lm_distribution import TextSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "        TextSample(list(), \"This is quite amazing.\"),\n",
    "        TextSample(list(), \"This is amazingly relevant.\"),\n",
    "        TextSample(list(), \"This is the toolkit at work.\")\n",
    "    ]\n",
    "amazing_scorer.log_score(samples, '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The lists as first members of the tuples are empty as we don't need the tokenized form to score the samples with amazing_scorer.\n",
    "1. We pass an empty context when (log-)scoring as it's not relevant when looking for amazing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we've again simplied things a bit here as we're not scoring the presence of \"amazing\" as word but of the string. Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_amazing = lambda s, c: bool(re.search(r\"\\bamazing\\b\", s.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_scorer = BooleanScorer(is_amazing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_scorer.log_score(samples, '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this scorer, we can express our preference in an EBM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by instantiating a LMDistribution as the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions import LMDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = LMDistribution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify that all our generated samples should include the word \"amazing\", we then use a straight product to define our target EBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * amazing_scorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we've only specified our preferences, or constraints: we would then have to approximate this EBM, for example by tuning a model —although we've expressed that we want all our samples to include \"amazing\", we will only approach that constraint, up to about 80% from our experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Constraint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want half of our samples to include amazing, we have to constrain the base model to compute the coefficient to use in the resulting EBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer], [1/2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works we've hidden something with this default syntax: the coefficients are computed for an empty context which might not be what we want to do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify a fixed context, other than the empty string we have to use a `SingleContextDistribution`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions.single_context_distribution import SingleContextDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incipit = \"It was a cold and stormy night\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer], [1/2],\n",
    "        n_samples=2**10,\n",
    "        context_distribution=SingleContextDistribution(incipit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to compute those coefficients for variable contexts, we can use a ContextDistribution to specify a text file listing them, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions.context_distribution import ContextDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer], [1/2],\n",
    "        n_samples=2**9,\n",
    "        context_distribution=ContextDistribution(\"data/incipits.txt\"), n_contexts_per_step=2**3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we peek inside the EBM we can check the coefficient that's been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.scorers[1].coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this knowledge, we could define our EBM directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.scorers.exponential_scorer import ExponentialScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * ExponentialScorer([is_amazing], [6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Scorers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Scorers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go beyond the very simple example of looking for \"amazing\" in our samples.  \n",
    "A first thing we can do is have multiple such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_rainy = lambda s, c: bool(re.search(r\"\\brain\\b\", s.text))\n",
    "rainy_scorer = BooleanScorer(is_rainy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this second scorer, we can constrain our base model, wishing for 50% of the samples with the word \"amazing\" and 33% with the \"rain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer, rainy_scorer], [1/2, 1/3],\n",
    "        n_samples=2**10,\n",
    "        context_distribution=SingleContextDistribution(incipit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-based Scorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Readability</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try something a bit ambitious than just looking for the presence of \"amazing\", be it the string or the word. What about readability? If we can score our samples using for example a FOG index we might use that as a feature, a preference expressed in an EBM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are useful packages available to compute readability index but we can try to define our own functions: the [FOG](https://en.wikipedia.org/wiki/Gunning_fog_index) is a classic measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "def count_sentences(text):\n",
    "    if \"\" == text:\n",
    "        return 0\n",
    "    marks = set(\".!?\")\n",
    "    rw_lngth = len([l for l in text if l in marks])\n",
    "    return rw_lngth if 0 < rw_lngth else 1\n",
    "\n",
    "def count_syllables(word):\n",
    "    vowels = set(\"aeiou\")\n",
    "    return len([l for l in word if l.lower() in vowels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog(text):\n",
    "    l, h = 6, 17\n",
    "    if \"\" == text:\n",
    "        return l\n",
    "    wrds = extract_words(text)\n",
    "    n_wrds = len(wrds)\n",
    "    n_cmplx_wrds = len([w for w in wrds if 3 < count_syllables(w)])\n",
    "    n_sntncs = count_sentences(text)\n",
    "    rw_scr = round(0.4 * (n_wrds / n_sntncs + 100 * n_cmplx_wrds / n_wrds))\n",
    "    return min(h, max(l, rw_scr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars = \"\"\"It is a period of civil war. Rebel spaceships,\n",
    "striking from a hidden base, have won their first victory against\n",
    "the evil Galactic Empire. During the battle, Rebel spies managed\n",
    "to steal secret plans to the Empire’s ultimate weapon, the DEATH STAR,\n",
    "an armoured space station with enough power to destroy an entire planet.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog(star_wars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to use this `fog()` is to pass is to a BooleanScorer, in a scoring predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * BooleanScorer(lambda s, c: True if 13 > fog(s.text) else False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, to make things a bit clearer, if a bit more verbose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy(s, _):\n",
    "    \"\"\"a FOG index lower than 13 means that the text\n",
    "    should be readable without college education\"\"\"\n",
    "    return True if 13 > fog(s.text) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * BooleanScorer(easy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what that gives for a few samples from a default GPT-2, using our infamous [incipit](https://en.wikipedia.org/wiki/It_was_a_dark_and_stormy_night)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = LMDistribution()\n",
    "samples, _ = proposal.sample(context=incipit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.log_score(samples, context=incipit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined pointwise constraints with our products so we expect all generated samples to be easily readable.  \n",
    "Obviously we could define a distributional constraints, asking for only half of our samples to be easy for example. What would make even more sense here is state that we want our sentences to have _on average_ a FOG index corresponding to the end of high school."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we're going to use a more generic `PositiveScorer` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.scorers.positive_scorer import PositiveScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([PositiveScorer(lambda s, c: fog(s.text))], [12],\n",
    "    context_distribution=SingleContextDistribution(incipit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, again, using a file of incipits for a variable contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([PositiveScorer(lambda s, c: fog(s.text))], [12],\n",
    "        n_samples=2**9,\n",
    "        context_distribution=ContextDistribution(\"data/incipits.txt\"), n_contexts_per_step=2**3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another way to define our feature is to subclass a `PositiveScorer` in our own `FogScorer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FogScorer(PositiveScorer):\n",
    "    \"\"\"\n",
    "    FOG scoring class\n",
    "    \"\"\"\n",
    "\n",
    "    def _extract_words(self, text):\n",
    "        return re.findall(r'\\w+', text)\n",
    "\n",
    "    def _count_sentences(self, text):\n",
    "        if \"\" == text:\n",
    "            return 0\n",
    "        marks = set(\".!?\")\n",
    "        rw_lngth = len([l for l in text if l in marks])\n",
    "        return rw_lngth if 0 < rw_lngth else 1\n",
    "\n",
    "    def _count_syllables(self, word):\n",
    "        vowels = set(\"aeiou\")\n",
    "        return len([l for l in word if l.lower() in vowels])\n",
    "\n",
    "    def fog(self, sample, _):\n",
    "        text = sample.text\n",
    "        l, h = 6, 17\n",
    "        if \"\" == text:\n",
    "            return l\n",
    "        wrds = self._extract_words(text)\n",
    "        n_wrds = len(wrds)\n",
    "        n_cmplx_wrds = len([w for w in wrds if 3 < self._count_syllables(w)])\n",
    "        n_sntncs = self._count_sentences(text)\n",
    "        rw_scr = round(0.4 * (n_wrds / n_sntncs + 100 * n_cmplx_wrds / n_wrds))\n",
    "        return min(h, max(l, rw_scr))\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scoring_function = self._broadcast(self.fog)\n",
    "\n",
    "    def log_score(self, samples, context):\n",
    "        return torch.log(self.score(samples, context))\n",
    "\n",
    "    def score(self, samples, context):\n",
    "        return torch.tensor(self.scoring_function(samples, context))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be used very similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([FogScorer()], [12],\n",
    "        n_samples=2**9,\n",
    "        context_distribution=ContextDistribution(\"data/incipits.txt\"), n_contexts_per_step=2**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "babb4baf4e80bd80b9852210fc5469c0783907e52a560ed7247caef52808358d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
